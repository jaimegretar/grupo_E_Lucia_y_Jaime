---
title: "Suicidio y climas extremos"
author: Jaime Miguel y Lucía Toimil
date: Sys.Date()
output: html_document:
    toc: true
    toc_float: true
---

# INTRODUCCIÓN
**Islandia** se caracteriza por su clima frio y extremo, además de por la escasa cantidad de luz durante gran parte del año. En cambio, **Arizona** tiene un clima cálido y seco, con abundante luz solar la mayor parte del tiempo.
Estos dos lugares, con climas tan extremos y distintos, permiten evaluar de manera comparativa como diferentes condiciones climáticas pueden influir en la **salud mental** y en la tasa de **suicidios**.

# OBJETIVOS
## Objetivo general
Analizar la posible relación entre las condiciones climáticas y la salud mental, evaluando como el clima podría influir en la tasa de suicidios en Islandia y Arizona.

# Objetivos especificos
- Comparar la tasa de suicidios en Islandia y Arizona en relación con sus caracteristicas climáticas.
- Comparar las tasas de suicidios entre hombres y mujeres en ambos lugares
- Estudiar el impacto de episodios extremos de temperatura con suicidios posteriores a ellos.
- Analizar efectos estacionales en las tasas de suicidios.

# METODOLOGÍA
## 1.1. Carga de paquetes:
A continuación se encuentran los paquetes empleados para cargar y trabajar con los datos del seminario.
```{r setup, message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)
library(lubridate)
library(rjstat)     
library(jsonlite)
library(rjson)
library(tidyjson)
library(plotly)
library(dplyr)
```
## 1.2. Obtención de datos de temperatura y suicidio (Islandia y Arizona)
Para la obtencion de datos hemos realizado diversas busquedas en bases de datos y consultas a una API.
El siguiente código Python define las funciones necesarias para consultar la API de Veðurstofa Íslands (api.vedur.is), buscar la estación por nombre ("Reykjav"), solicitar la temperatura media diaria (t) y guardar el resultado en archivos CSV y JSON. La consulta se ha realizado con ayuda de la IA.
```{python}
import requests
import pandas as pd
from datetime import datetime, date

BASE = "https://api.vedur.is/weather"
CITY_HINT = "Reykjav"              # Cambia por "Akureyri" si quieres esa zona
START = "2018-01-01"               # Puedes ampliar a 2015-01-01 cuando veas cobertura
END   = "2023-12-31"
OUT_PREFIX = "temperatura_reykjavik_2018_2023"
HEADERS = {"Accept": "application/json"}

def normalize_rows(payload):
    if isinstance(payload, list):
        return payload
    if isinstance(payload, dict) and isinstance(payload.get("data"), list):
        return payload["data"]
    if isinstance(payload, dict):
        for v in payload.values():
            if isinstance(v, list):
                return v
    return []

def find_station_id_by_name_hint(name_hint: str) -> int:
    """Busca estación por nombre en /observations/synop/latest"""
    url = f"{BASE}/observations/synop/latest"
    r = requests.get(url, params={"format":"json"}, headers=HEADERS, timeout=60)
    r.raise_for_status()
    rows = normalize_rows(r.json())
    # buscar coincidencia por nombre
    matches = [row for row in rows
               if isinstance(row, dict)
               and isinstance(row.get("name"), str)
               and name_hint.lower() in row["name"].lower()]
    if not matches:
        # Si no hay, intenta devolver la primera de la capital region (region_id=1)
        url2 = f"{BASE}/observations/synop/latest"
        r2 = requests.get(url2, params={"format":"json", "region_id":"1"}, headers=HEADERS, timeout=60)
        r2.raise_for_status()
        rows2 = normalize_rows(r2.json())
        if not rows2:
            raise RuntimeError("No pude listar estaciones (latest).")
        print("⚠ No encontré por nombre; uso primera de la región 1.")
        return int(rows2[0]["station"])
    station_id = int(matches[0]["station"])
    print(f"✔ Estación encontrada: '{matches[0]['name']}' (id={station_id})")
    return station_id

def year_chunks(start: str, end: str):
    sd = datetime.fromisoformat(start).date()
    ed = datetime.fromisoformat(end).date()
    for year in range(sd.year, ed.year + 1):
        a = date(year, 1, 1)
        b = date(year, 12, 31)
        if a < sd: a = sd
        if b > ed: b = ed
        yield a, b

def fetch_synop_agg(aggregation: str, station_id: int, day_from: date, day_to: date) -> pd.DataFrame:
    """
    Llama a /observations/synop/{aggregation} usando station_id como array en query:
    station_id=1&station_id=1  (requests admite lista de tuplas para repetir keys)
    """
    assert aggregation in {"day","month","year","clock"}
    endpoint = f"{BASE}/observations/synop/{aggregation}"
    # lista de tuplas para que 'station_id' salga repetido (array)
    params = [
        ("station_id", station_id),            # array<integer>
        ("day_from", day_from.isoformat()),
        ("day_to",   day_to.isoformat()),
        ("parameters", "basic"),
        ("format", "json"),
        ("order",  "asc"),
    ]
    r = requests.get(endpoint, params=params, headers=HEADERS, timeout=120)
    if r.status_code == 200:
        rows = normalize_rows(r.json())
        return pd.DataFrame(rows)
    # imprime ayuda de depuración
    raise RuntimeError(f"{endpoint} devolvió {r.status_code}: {r.text[:200]}")

def get_daily_t(station_id: int, start: str, end: str) -> pd.DataFrame:
    frames = []
    for a, b in year_chunks(start, end):
        try:
            df = fetch_synop_agg("day", station_id, a, b)
            if df.empty:
                print(f"⚠ Día {a}–{b}: sin filas")
                continue
            # Esperado: columns year, month, day, t (según el esquema)
            required = {"year","month","day","t"}
            if not required.issubset(df.columns):
                raise RuntimeError(f"Estructura inesperada (day). Columnas: {df.columns.tolist()}")
            df["fecha"] = pd.to_datetime(df[["year","month","day"]])
            frames.append(df[["fecha","t"]].rename(columns={"t":"temperatura_media"}).dropna())
            print(f"✔ Día {a}–{b}: {len(df)} filas")
        except Exception as e:
            print(f"❌ Día {a}–{b} error: {e}")
    if not frames:
        return pd.DataFrame(columns=["fecha","temperatura_media"])
    out = (pd.concat(frames, ignore_index=True)
             .sort_values("fecha").reset_index(drop=True))
    return out

def get_monthly_t(station_id: int, start: str, end: str) -> pd.DataFrame:
    frames = []
    for a, b in year_chunks(start, end):
        try:
            df = fetch_synop_agg("month", station_id, a, b)
            if df.empty:
                print(f"⚠ Mes {a}–{b}: sin filas")
                continue
            required = {"year","month","t"}
            if not required.issubset(df.columns):
                raise RuntimeError(f"Estructura inesperada (month). Columnas: {df.columns.tolist()}")
            # para mensual no hay 'day'; ponemos día 1
            df["fecha"] = pd.to_datetime(df[["year","month"]].assign(day=1))
            frames.append(df[["fecha","t"]].rename(columns={"t":"temperatura_media"}).dropna())
            print(f"✔ Mes {a}–{b}: {len(df)} filas")
        except Exception as e:
            print(f"❌ Mes {a}–{b} error: {e}")
    if not frames:
        return pd.DataFrame(columns=["fecha","temperatura_media"])
    out = (pd.concat(frames, ignore_index=True)
             .sort_values("fecha").reset_index(drop=True))
    return out

def save(df: pd.DataFrame, prefix: str):
    df.to_csv(f"{prefix}.csv", index=False, encoding="utf-8")
    df.to_json(f"{prefix}.json", orient="records", indent=2, date_format="iso")
    print("\n✅ Guardado en:")
    print(f" - {prefix}.csv")
    print(f" - {prefix}.json")

if __name__ == "__main__":
    station_id = find_station_id_by_name_hint(CITY_HINT)

    # 1) Intento diario
    daily = get_daily_t(station_id, START, END)

    if not daily.empty:
        print(daily.head())
        save(daily, OUT_PREFIX)                      # salida diaria
    else:
        print("↩️ No hubo datos diarios; paso a semanal (re-muestreo) o mensual.")
        # 2a) Intento semanal a partir de 'clock' o 'day' => si no hay 'day', usa 'month'
        monthly = get_monthly_t(station_id, START, END)
        if monthly.empty:
            raise SystemExit("❌ No hubo datos diarios ni mensuales en el rango. Prueba otra estación o rango.")
        save(monthly, OUT_PREFIX + "_mensual")
        print("ℹ He guardado la serie mensual. Si quieres, te genero semanal a partir de la mensual.")
```

### 1.2.1 Importacion de datos de temperatura:
#### Arizona:
Obtenemos los datos de temperatura de Arizona, desde un archivo .csv y desde un archivo .json:
```{r}
# Importacion datos temperatura Arizona .json
Arizona_temp_json <- fromJSON(file = "INPUT/DATA/Arizona/Temperatura/data.json")
Arizona_temp_json %>%
  spread_all() %>%
  View()

# Importacion .csv datos temperatura Arizona
Arizona_temp_csv <- read_delim(file="INPUT/DATA/Arizona/Temperatura/data.csv", delim = ",")
View(Arizona_temp_csv)
```
Aunque disponemos de los datos de temperatura en ambos formatos, optaremos por utilizar el archivo .csv, ya que es más sencillo de manejar y el formato .json nos ha generado problemas.

#### Islandia:
Importacion de los datos de temperatura de Islandia (Reikiavik) en .json, en el periodo del 2018-2023:
```{r}
# Importacion datos temperatura Islandia .json
Islandia_temp_json <- fromJSON(file = "INPUT/DATA/Islandia/Temperatura/temperatura_reykjavik_2018_2023.json")
Islandia_temp_json %>%
  spread_all() %>%
  View()
```
### 1.2.2. Importacion de datos de suicidios:
#### Arizona:
Importamos los datos de suicidios de Arizona (relacionados con salud mental) desde un archivo CSV. Se especifican los nombres de las columnas y se salta la fila de encabezado que nos genera problemas en la importación.
```{r}
Arizona_suicidioRegion_csv <- read_csv(
  file = "INPUT/DATA/Arizona/Salud/HDPulse_data_export.csv",
  col_names = c(
    "Region/Condado",
    "Codigo_FIPS",
    "Muertes_100,000",
    "CI_Inferior",
    "CI_Superior",
    "Num_Muertes_Anual",
    "Tendencia",
    "Tendencia_Anual",
    "CI_Inferior_rep",
    "CI_Superior_rep"
  ),
  skip = 1,              # Para saltar la fila de encabezado que da errores
  trim_ws = TRUE,
  show_col_types = FALSE
)

View(Arizona_suicidioRegion_csv)
```
Otro archivo .csv para complementar los datos de suicidios de Arizona desde el 2018-2023 (se utilizo la ayuda de la IA para poder importar el archivo correctamente ya que contenia los datos encapsulados con dobles comillas y más caracteres que dificultaban su procesamiento):
```{r}
# Importacion suicidios Arizona 2 del 2018 al 2023
datos2 <- read.csv("INPUT/DATA/Arizona/Salud/reports-data-export.csv",
                   header = TRUE,
                   stringsAsFactors = FALSE)

# Usar una expresión regular sofisticada para separar respetando las comillas
datos_separados <- datos2 %>%
  mutate(linea = .[[1]]) %>%
  separate(linea,
           into = c("Intent", "Year", "Age_Group", "Sex", "Deaths",
                    "Population", "Crude_Rate", "Age_Adjusted_Rate",
                    "Years_Potential_Life_Lost"),
           sep = ',(?=(?:[^"]*"[^"]*")*[^"]*$)', # Separar por comas fuera de comillas
           extra = "merge") %>%
  mutate(across(everything(), ~str_replace_all(., '"', ''))) %>% # Quitar comillas
  select(-1) # Eliminar la primera columna original

# Ver resultado
View(datos_separados)
```
#### Islandia:
Importamos los datos de suicidios de Islandia desde un archivo JSON con formato JSON-stat.
```{r}
# Importacion .json datos suicidios de Islandia (relacionado con salud mental)
Islandia_suicidio_json <- fromJSONstat("INPUT/DATA/Islandia/Salud/suicidios_islandia.json")
df <- as.data.frame(Islandia_suicidio_json)
View(df)
```

## 1.3. Limpieza y preparación de datos 
Los archivos importados contienen mucha información que no es importante para nuestro seminario. Por lo que, vamos a filtrar todos los archivos para quedarnos unicamente con datos relacionados con suicidios y temperaturas, concretamente entre los años 2018 y 2023.
### 1.3.1. Limpieza y preparación de datos de temperatura
#### Arizona:
Filtramos del Arizona_temp_csv los datos de temperatura media entre los años 2018 y 2023, y creamos nuevas columnas para el año, mes, estación del año, etc.
```{r}
Arizona_temp_filtrado <- Arizona_temp_csv %>%
  filter(Date >= 201801 & Date <= 202312)

View(Arizona_temp_filtrado)
```
#### Islandia:
En el caso de Islandia , los datos ya están filtrados en el archivo JSON, por lo que no es necesario realizar un filtrado adicional.

### 1.3.2. Limpieza y preparación de datos de suicidios
#### Arizona:
Filtramos los datos de suicidios de Arizona pero solo del segundo csv ya que el primero es por regiones y no especifica los años.
Primeramente hay que convertir los datos a tipo numer ico para poder trabajar con ellos.
```{r}
# Limpieza de datos - convertir a numérico
arizona_csv <- datos_separados %>%
  mutate(
    Year = as.numeric(Year),
    Deaths = as.numeric(gsub("[^0-9]", "", Deaths)),
    Population = as.numeric(gsub(",", "", Population)),  # Solo quitar comas
    Crude_Rate = as.numeric(gsub("[*]", "", Crude_Rate)),
    Age_Adjusted_Rate = as.numeric(gsub("[*]", "", Age_Adjusted_Rate)),
    Years_Potential_Life_Lost = as.numeric(gsub(",", "", Years_Potential_Life_Lost))
  )

# Ver resumen
View(arizona_csv)
```
Posteriormente, filtramos solo los suicidios entre los años 2018 y 2023.
```{r}
# Filtrar solo suicidios
arizona_suicidios <- arizona_csv %>%
  filter(Intent == "Suicide")

# Ver el resultado
View(arizona_suicidios)
```
#### Islandia:
Filtramos los suicidios en Islandia entre los años 2018 y 2023.
```{r}
suicidios_Islandia <- df %>%
  mutate(Year = as.numeric(Year)) %>%
  filter(Year >= 2018 & Year <= 2023)

# Ver años únicos
unique(suicidios_Islandia$Year)

# Contar observaciones por año
suicidios_Islandia %>%
  count(Year)
view(suicidios_Islandia)
```
